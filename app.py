# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1drnKsznLLWGyCmbxZtP_Y85UQIvkyT5y
"""

import streamlit as st
import pandas as pd
import numpy as np
import shap
import dice_ml
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# ==========================================
# 1. SETUP & DYNAMIC DATA LOADING
# ==========================================
@st.cache_resource
def load_data_and_train_model():
    try:
        # Load the dataset
        df = pd.read_csv("kidney_dataset.csv")

        # CLEANING: Remove 'id' column if it exists, as it's not a feature
        if 'id' in df.columns:
            df = df.drop(columns=['id'])

    except FileNotFoundError:
        # FALLBACK: Generate realistic Standard CKD columns if CSV is missing
        # This ensures the app doesn't crash on the first run
        np.random.seed(42)
        rows = 100
        data = {
            'age': np.random.randint(20, 90, rows),
            'bp': np.random.randint(50, 110, rows),           # Blood Pressure
            'sg': np.random.choice([1.005, 1.010, 1.015, 1.020, 1.025], rows), # Specific Gravity
            'al': np.random.choice([0, 1, 2, 3, 4, 5], rows), # Albumin
            'su': np.random.choice([0, 1, 2, 3, 4, 5], rows), # Sugar
            'rbc': np.random.choice(['normal', 'abnormal'], rows),
            'pc': np.random.choice(['normal', 'abnormal'], rows),
            'pcc': np.random.choice(['present', 'notpresent'], rows),
            'ba': np.random.choice(['present', 'notpresent'], rows),
            'bgr': np.random.uniform(70, 490, rows),          # Blood Glucose Random
            'bu': np.random.uniform(10, 390, rows),           # Blood Urea
            'sc': np.random.uniform(0.4, 15, rows),           # Serum Creatinine
            'sod': np.random.uniform(100, 160, rows),         # Sodium
            'pot': np.random.uniform(2.5, 7.5, rows),         # Potassium
            'hemo': np.random.uniform(3, 17, rows),           # Hemoglobin
            'pcv': np.random.randint(20, 50, rows),           # Packed Cell Volume
            'wc': np.random.randint(2000, 20000, rows),       # WBC Count
            'rc': np.random.uniform(2, 8, rows),              # RBC Count
            'htn': np.random.choice(['yes', 'no'], rows),     # Hypertension
            'dm': np.random.choice(['yes', 'no'], rows),      # Diabetes
            'cad': np.random.choice(['yes', 'no'], rows),     # Coronary Artery Disease
            'appet': np.random.choice(['good', 'poor'], rows),# Appetite
            'pe': np.random.choice(['yes', 'no'], rows),      # Pedal Edema
            'ane': np.random.choice(['yes', 'no'], rows),     # Anemia
            'classification': np.random.choice(['ckd', 'notckd'], rows) # Target
        }
        df = pd.DataFrame(data)

    # DETECT TARGET COLUMN (It's usually 'classification' or 'class' or 'CKD_Status')
    target = None
    possible_targets = ['classification', 'class', 'CKD_Status', 'target', 'diagnosis']
    for t in possible_targets:
        if t in df.columns:
            target = t
            break

    if target is None:
        target = df.columns[-1] # Fallback to last column

    # Clean Target: Map to 0 and 1 if it's string (ckd/notckd)
    if df[target].dtype == 'object':
        df[target] = df[target].astype(str).str.lower().map({'ckd': 1, 'notckd': 0, 'yes': 1, 'no': 0, '1': 1, '0': 0})
        # Drop rows where target became NaN (invalid labels)
        df = df.dropna(subset=[target])
        df[target] = df[target].astype(int)

    # SEPARATE FEATURES AND TARGET
    X = df.drop(columns=[target])
    y = df[target]

    # Preprocessing Pipelines
    numeric_cols = X.select_dtypes(include=['number']).columns
    categorical_cols = X.select_dtypes(include=['object']).columns

    preprocessor = ColumnTransformer([
        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),
        ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_cols)
    ])

    # Model
    model = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model.fit(X_train, y_train)

    return model, X_train, df, numeric_cols, categorical_cols

# Load Resources
model_pipeline, X_train, df_full, num_cols, cat_cols = load_data_and_train_model()

# ==========================================
# 2. EXPLANATION LOGIC (SHAP & DiCE)
# ==========================================
def generate_shap_explanation(model, X_train, instance):
    try:
        preprocessor = model.named_steps['preprocessor']
        classifier = model.named_steps['classifier']

        # Transform
        instance_trans = preprocessor.transform(instance)

        # Calculate SHAP
        explainer = shap.TreeExplainer(classifier)
        shap_values = explainer.shap_values(instance_trans)

        # Handle Binary Classification
        if isinstance(shap_values, list):
            vals = shap_values[1] # Class 1 (Risk)
        else:
            vals = shap_values

        vals = np.array(vals).flatten()

        # Extract Feature Names
        # Note: This relies on the transformers being named 'num' and 'cat'
        num_names = preprocessor.named_transformers_['num'].get_feature_names_out().tolist()
        cat_names = preprocessor.named_transformers_['cat'].get_feature_names_out().tolist()
        feature_names = num_names + cat_names

        # Safety Trim
        min_len = min(len(vals), len(feature_names))
        vals = vals[:min_len]
        feature_names = feature_names[:min_len]

        # Create DataFrame
        feature_importance = pd.DataFrame({'col_name': feature_names, 'feature_importance_vals': vals})
        feature_importance['abs_val'] = feature_importance['feature_importance_vals'].abs()
        feature_importance = feature_importance.sort_values(by='abs_val', ascending=False).head(3)

        text = "Based on the AI analysis, the **top 3 factors** driving this prediction are:\n"
        for _, row in feature_importance.iterrows():
            # Clean up feature names (remove "num__" or "cat__")
            clean_name = row['col_name'].replace("num__", "").replace("cat__", "")
            impact = "Increases Risk üî∫" if row['feature_importance_vals'] > 0 else "Reduces Risk üîª"
            text += f"- **{clean_name}**: {impact}\n"
        return text, feature_importance

    except Exception as e:
        return f"Could not generate SHAP explanation. (Error: {str(e)})", None

def generate_dice_counterfactual(model, df_full, instance):
    try:
        # Re-identify target column name from the full dataframe
        target_name = df_full.columns[-1] # Assuming target is last
        if 'classification' in df_full.columns: target_name = 'classification'
        elif 'class' in df_full.columns: target_name = 'class'
        elif 'CKD_Status' in df_full.columns: target_name = 'CKD_Status'

        # Setup DiCE
        d = dice_ml.Data(dataframe=df_full, continuous_features=num_cols.tolist(), outcome_name=target_name)
        m = dice_ml.Model(model=model, backend="sklearn")
        exp = dice_ml.Dice(d, m, method="random")

        # Generate
        e1 = exp.generate_counterfactuals(instance, total_CFs=1, desired_class=0)
        cf_df = e1.cf_examples_list[0].final_cfs_df

        return "To potentially reverse the risk, consider aiming for these values:", cf_df
    except Exception as e:
        return f"No simple actionable changes found for this case. (Error: {str(e)})", None

# ==========================================
# 3. STREAMLIT UI
# ==========================================
st.set_page_config(layout="wide", page_title="Medical AI Assistant")

st.sidebar.header("üìù Patient Vitals")
st.sidebar.markdown("The inputs below are **automatically generated** based on your dataset columns.")

# --- DYNAMIC INPUT GENERATION ---
user_inputs = {}

# 1. Numeric Inputs (Sliders)
for col in num_cols:
    # Calculate min, max, and mean for default values
    min_val = float(df_full[col].min())
    max_val = float(df_full[col].max())
    mean_val = float(df_full[col].mean())

    # Create slider
    user_inputs[col] = st.sidebar.slider(
        label=col.capitalize(),
        min_value=min_val,
        max_value=max_val,
        value=mean_val
    )

# 2. Categorical Inputs (Dropdowns)
for col in cat_cols:
    # Get unique options
    options = df_full[col].unique().tolist()
    # Create dropdown
    user_inputs[col] = st.sidebar.selectbox(
        label=col.capitalize(),
        options=options
    )

# Convert dictionary to DataFrame for prediction
input_df = pd.DataFrame([user_inputs])

# --- MAIN DASHBOARD ---
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("Patient Profile")
    st.markdown("Overview of the data entered in the sidebar:")
    # Transpose for easier reading
    st.dataframe(input_df.astype(str).T, use_container_width=True)

with col2:
    st.subheader("Diagnosis & Explanation")

    if st.button("Run Diagnostics", type="primary"):
        # Predict
        pred = model_pipeline.predict(input_df)[0]
        prob = model_pipeline.predict_proba(input_df)[0][1]

        st.session_state['pred'] = pred
        st.session_state['prob'] = prob
        st.session_state['run'] = True

    if st.session_state.get('run'):
        # Display Prediction
        if st.session_state['pred'] == 1:
            st.error(f"‚ö†Ô∏è **High Risk Detected** (Confidence: {st.session_state['prob']:.1%})")
        else:
            st.success(f"‚úÖ **Low Risk Detected** (Confidence: {(1-st.session_state['prob']):.1%})")

        st.divider()

        # Interactive Q&A
        st.markdown("### üí¨ AI Consultation")
        st.info("Ask questions in plain English. Examples: *'Why is the risk high?'*, *'What changes can I make?'*")

        user_q = st.text_input("Type your question here:")

        if user_q:
            q = user_q.lower()

            # INTENT: Feature Importance (SHAP)
            if any(x in q for x in ["why", "reason", "cause", "factor", "attribute"]):
                st.markdown("#### üîç Root Cause Analysis (SHAP)")
                with st.spinner("Analyzing patient data..."):
                    txt, sdf = generate_shap_explanation(model_pipeline, X_train, input_df)
                    st.write(txt)
                    if sdf is not None:
                        # Clean names for chart
                        sdf['clean_name'] = sdf['col_name'].str.replace('num__', '').str.replace('cat__', '')
                        st.bar_chart(sdf.set_index('clean_name')['feature_importance_vals'])

            # INTENT: Counterfactuals (DiCE)
            elif any(x in q for x in ["change", "how to", "what if", "reduce", "improve"]):
                st.markdown("#### üîÆ Actionable Insights (DiCE)")
                if st.session_state['pred'] == 0:
                    st.write("Patient is already Low Risk. No changes needed.")
                else:
                    with st.spinner("Calculating recovery path..."):
                        txt, cdf = generate_dice_counterfactual(model_pipeline, df_full, input_df)
                        st.write(txt)
                        if cdf is not None:
                            st.dataframe(cdf)

            else:
                st.warning("I didn't understand. Try asking about 'causes' or 'changes'.")